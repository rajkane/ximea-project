# XIMEA Project

Desktop GUI application for building an image dataset, training a Faster R-CNN model, and running object detection/inspection.

> This repository keeps the application code in `src/`. The `Dataset/` and `Models/` folders are runtime folders (kept as empty directories in git via `.gitkeep`).

## What is this app for?

This app is meant to support an end-to-end workflow:

1. **Capture / collect images** (e.g., from a Ximea camera)
2. **Annotate images** in an external labeling tool (Pascal VOC **XML**)
3. **Train** an object detector (Faster R-CNN)
4. **Run inference** (object detection + inspection) from the GUI

Screenshots:

- Dataset creation / filling:

  <img src="./fill_dataset.png" alt="dataset" width="800" height="auto">

- Training (learning) view:

  <img src="./learning.png" alt="learning" width="800" height="auto">

- Main detection / inspection view:

  <img src="./main.png" alt="main" width="800" height="auto">

## Hardware

- Camera: **Ximea MU050CR-SY**

## Tech stack

- Ximea camera API
- PyQt6 (GUI)
- OpenCV
- PyTorch / Torchvision (Faster R-CNN)
- `detecto` utilities (dataset helpers)

## Project structure

- `src/` — application source code (MVC-style split into `model/`, `view/`, `controller/`)
- `Dataset/` — runtime dataset folder (not versioned; only the folder exists in git)
  - expected: `Dataset/<dataset_name>/train/` and `Dataset/<dataset_name>/valid/`
- `Models/` — runtime trained models folder (not versioned; only the folder exists in git)
  - saved model weights: `Models/<model_name>.pth`
  - saved model classes/annotations: `Models/<model_name>.pth.txt`

## Dataset & annotations

- Images are typically `*.jpeg`.
- Annotations are expected as Pascal VOC **XML** files (`*.xml`) generated by your labeling tool.
- Folder layout (example):

  - `Dataset/cap_dataset/train/…` (images + xml)
  - `Dataset/cap_dataset/valid/…` (images + xml)

### Annotation labels

In the GUI you can enter multiple labels as a **comma-separated list**, for example:

- `Cap OK, Cap NOK`

Internally the app converts this into a Python list of classes. During training the app validates that labels in XML match the labels provided in the GUI.

## Installation

Create and activate a virtual environment, then install dependencies:

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

## Run

Start the GUI:

```bash
python src/main.py
```

## Training output

When training finishes, the app saves:

- `Models/<model_name>.pth` — model weights
- `Models/<model_name>.pth.txt` — a single line containing classes, e.g. `Cap OK, Cap NOK`

## Inference backend (CUDA / CPU / ONNX)

The app can run inference on:

- **PyTorch + CUDA** (fastest, if you have an NVIDIA GPU and CUDA working)
- **PyTorch CPU** (works everywhere, slower)
- **ONNX CPU** (optional, *only if you later add ONNX export + onnxruntime inference*)

### Automatic selection (recommended)

Default is **AUTO**:

1. Use **CUDA** if available
2. Else use **ONNX CPU** if `onnxruntime` is installed
3. Else use **PyTorch CPU**

The resolved backend is stored in env var `INFERENCE_BACKEND_RESOLVED`.

### Manual selection (all options)

You can force a backend in two ways.

**1) Environment variable** (works for GUI launchers too):

- `INFERENCE_BACKEND=auto`
- `INFERENCE_BACKEND=torch-cuda`
- `INFERENCE_BACKEND=torch-cpu`
- `INFERENCE_BACKEND=onnx-cpu`

**2) CLI argument**:

- `python src/main.py --backend auto`
- `python src/main.py --backend torch-cuda`
- `python src/main.py --backend torch-cpu`
- `python src/main.py --backend onnx-cpu`

Notes:
- If you force `torch-cpu`, the app also sets `CUDA_VISIBLE_DEVICES=''` to prevent accidental CUDA usage.
- If you request a backend that is not available, the app falls back to **AUTO**.
- Selecting `onnx-cpu` currently only selects the backend (for future wiring). Actual ONNX inference will work only after ONNX export + runtime support is implemented.

### Is the ONNX model quantized?

No — exporting to **ONNX** does **not** automatically quantize the model.

You can inspect an ONNX file and see whether it contains INT8/UINT8 weight tensors using:

```bash
python -m src.model.inspect_onnx_model Models/cap_model.onnx
```

Output example:

- `is_quantized: False` + `initializer_dtypes: FLOAT=...` → the model is **not int8-quantized** (typically float32)
- `is_quantized: True` + `initializer_dtypes: INT8=...` or `UINT8=...` → the model is likely **int8-quantized**

If you want true INT8 quantization for ONNX, we would need to add an explicit quantization step (and possibly additional tooling). Ask before we add any extra packages for that.

## Camera fallback (no Ximea connected)

By default the app tries to open the configured **Ximea** camera first. If it is not available (not connected / driver not installed / wrong SN), the app automatically falls back to an **OpenCV** camera (**USB webcam / laptop camera**) and continues streaming.

### OpenCV camera index auto-scan

When falling back to OpenCV, the app can try multiple camera indices:

- `OPENCV_CAMERA_INDEX` — preferred index to try first (default `0`)
- `OPENCV_CAMERA_MAX_INDEX` — maximum index to try if the preferred one fails (default `3`)

Example (try indices 0..5):

```bash
OPENCV_CAMERA_INDEX=0 OPENCV_CAMERA_MAX_INDEX=5 python src/main.py
```

Tip: if your laptop camera is `0` and your USB camera is `1`, set `OPENCV_CAMERA_INDEX=1`.

## Troubleshooting

- **Training stops with unknown label error**: your XML files contain a label that is not listed in the GUI annotation field. Add the missing label(s) in the GUI or fix the XML annotations.
- **Slow training**: training on CPU can be very slow. A CUDA-capable GPU is recommended.
- **Camera not found**: verify Ximea drivers/API installation and camera connection.

## License

See `LISENCE`.
